{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Freiburg: A Workflow for Analyzing Large Corpora\n",
    "In this jupyter notebook I will cover the basics of performing stylometric analysis on a large collection of Chinese texts, whether they be modern or classical.\n",
    "## Using this notebook\n",
    "The code in this notebook is distributed across a few different code blocks. You will need to run them top to bottom, but the actual analysis does not happen until the last block. To run everything, simply click on \"Run All\" in the \"Cell\" menu. I have also provided a plain python file that you can run from the commmand line.\n",
    "\n",
    "## Importing necessary libraries\n",
    "This code block imports the libraries I will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os, sys, platform, json\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Plotting libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager\n",
    "import matplotlib.colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjustable parameters: Analysis\n",
    "The parameters that you might want to adjust for your analysis are contained in the following code block.\n",
    "\n",
    "**tokenizeMethod** can be set to the string 'char' or the string 'word'. This will determine how the document is tokenized. 'char' is much faster and useful for classical documents. 'word' depends on the jieba library. If it is not installed, the code will prompt you to install it. 'word' tokenization is better for modern Chinese.\n",
    "\n",
    "**ngrams** is an integer that will determine the size of n-gram you use for analysis. 1 will look at single characters (or words), 2 will look at two at a time, 3 will look at three at a time, etc. 1 grams work best for most analyses. More than 3 will be slow and often result in very sparse data that is hard to interpret.\n",
    "\n",
    "**commonWords** is an integer that determines how frequent a character must be in the corpus to be considered in the stylometric analysis. 500 will use the 500 most common words across all texts. You can set this to None if you do not want to limit words in this way\n",
    "\n",
    "**limitVocab** is a boolean (True or False). Set to True if you want to specify a specific vocabulary\n",
    "\n",
    "**limitVocabularyFile** is the name of a file that contains the vocabulary you are interested in. The file should have one token per line. This line is only read if limitVocab is set to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word or character tokenization?\n",
    "tokenizeMethod = \"char\"\n",
    "\n",
    "# Size of n-grams:\n",
    "ngrams = 1\n",
    "\n",
    "# Limit the number of words to look at\n",
    "commonWords = 500 \n",
    "\n",
    "# Set the vocabulary you are interested in\n",
    "limitVocab = False\n",
    "\n",
    "# Vocabulary file\n",
    "limitVocabularyFile = \"vocab.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjustable parameters: Appearance\n",
    "These parameters will help you set the appearance of the plot itself.\n",
    "\n",
    "**labelTypes** is a tuple that specifies the nature of the corpus labeling. Here, the sample corpus files are all  named with the convention title_dynasty_siku_sikusub_author.txt. Each type of label is one element in this tuple, in the same order they appear in the name\n",
    "\n",
    "**colorValue** this integer specifies which label should be used to generate a color scheme for the plot. 2 points to the 3rd element in the tuple, the siku categorization. There are three different siku categories reflected in the dataset, making this a good option. Here you should pick whichever label your analysis is focused on. More than 8 or so elements, however, will generate colors that are hard to tell apart.\n",
    "\n",
    "**labelValue** this integer specifies which label should be used for labeling the points in the plot. 0 points to the 1st element in the tuple, the title.\n",
    "\n",
    "**pointSize** is an integer that sets how large the points in the plot tare\n",
    "\n",
    "**pointLabels** is a boolean (True or False) that specifies if the points should be labeled.\n",
    "\n",
    "**plotLoadings** is a boolean that specifies if the vocabulary should be drawn on the plot (which will aid in interpretation). The further a term is from the center of the plot, the more it is influencing texts in a given direction.\n",
    "\n",
    "**hidePoints** is a boolean that specifies of the points should be drawn. Set to False to see the loadings better.\n",
    "\n",
    "**outputDimensions** is a tuple that sets the width and height of the output plot in inches. The inner values can be either integers or floats.\n",
    "\n",
    "**outputFile** contains the name of the outputfile, where the plot will be saved. The file extension will determine file type. png, pdf, jpg, tif, and others are all valid selections. On Macs, because of an oddity of the plotting library, pdfs will be very large. You can fix this by opening the file with adobe illustrator (or another similar program) and then saving a copy. This is because the entire font is embedded in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Types of labels for documents in the corpus\n",
    "labelTypes = ('title', 'dynasty', 'siku', 'subcat', 'author')\n",
    "\n",
    "# Index of label used to set Color:\n",
    "colorValue = 2 # Index of label to use for color. Here 2 points to \"siku\"\n",
    "\n",
    "# Index of label to use for plot labels (if points are labeled)\n",
    "labelValue = 0 # Index of label to use for labels. Here 0 points to \"title\"\n",
    "\n",
    "# Point size (set to integer)\n",
    "pointSize = 8\n",
    "\n",
    "# Show point labels (add labels for each text):\n",
    "pointLabels = False\n",
    "\n",
    "# Plot loadings (write the characters tot he plot)\n",
    "plotLoadings = False \n",
    "\n",
    "# Hide points (useful for seeing loadings better):\n",
    "hidePoints = False \n",
    "\n",
    "# Output file info (dimensions are in inches (width, height)):\n",
    "outputDimensions = (10, 7.5)\n",
    "\n",
    "# Output file extension determines output type. Save as a pdf if you want to edit in illustator\n",
    "# PDF Output on mac is very large, but just opening and saving a copy in illustrator will fix this\n",
    "outputFile = \"myfigure.png\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjustable Parameters: no need to change\n",
    "This parameters can be adjusted, but you may as well leave them as they are.\n",
    "\n",
    "**pcaComponents** is an integer that sets how many principal components should be calculated. We are only using two in this analysis, but as you work more with these plots, you can consider setting this higher (but you will also have to adjust later parts of the script to make them do anything). The maximum this can be is the number of variables (here the 500 words) minus one (so 499 in this case).\n",
    "\n",
    "**corpusFolder** is the name of the folder that holds the corpus files. Just leave this as \"corpus\" if you put your files in a folder called \"corpus\".\n",
    "\n",
    "**removeItemsFile** is a string that points to words (tokens) that you want to remove from consideration. Each token to be removed should be on a line in the specified file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many components?\n",
    "pcaComponents = 2 # Only useful for digging even deeper in the data\n",
    "\n",
    "# Input folder\n",
    "corpusFolder = \"corpus\"\n",
    "\n",
    "# Items to remove from consideration:\n",
    "removeItemsFile = \"remove.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nothing beyond here needs editing!!\n",
    "The comments in the code itself explain what is happening. If you run the script from a terminal, it will open a new window with your plot. It will look like the code keeps running until you close this window. This is an interactive explorer you can use to study the plot itself. Here it will just insert the figure after the codeblock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# Type Enforcement #\n",
    "####################\n",
    "\n",
    "# This section enforces the input values for all the adjustable variables. This\n",
    "# is to make sure the script isn't run incorrectly.\n",
    "\n",
    "# function to check values\n",
    "def valueChecker(varname, typeofobj, value):\n",
    "    if type(typeofobj) == type:\n",
    "        if typeofobj == bool and type(value) != typeofobj:\n",
    "            print(f\"{varname} must be a {typeofobj} (True or False). Please fix to run script.\")\n",
    "            sys.exit()\n",
    "        if type(value) != typeofobj:\n",
    "            print(f\"{varname} must be {typeofobj}. Please fix to run script.\")\n",
    "            sys.exec_info()\n",
    "            sys.exit() \n",
    "    elif type(typeofobj) == tuple:\n",
    "        if type(value) != typeofobj[0] and type(value) != typeofobj[1]:\n",
    "            print(f\"{varname} must be {typeofobj[0]} or {typeofobj[1]}. Please fix to run script.\")\n",
    "            sys.exit() \n",
    "\n",
    "# check values\n",
    "valueChecker('ngrams', int, ngrams)\n",
    "valueChecker('commonWords', (int, None), commonWords)\n",
    "valueChecker('limitVocab', bool, limitVocab)\n",
    "valueChecker('colorValue', int, colorValue)\n",
    "valueChecker('labelValue', int, labelValue)\n",
    "valueChecker('pointSize', int, pointSize)\n",
    "valueChecker('pointLabels', bool, pointLabels)\n",
    "valueChecker('plotLoadings', bool, plotLoadings)\n",
    "valueChecker('hidePoints', bool, hidePoints)\n",
    "valueChecker('outputFile', str, outputFile)\n",
    "valueChecker('pcaComponents', int, pcaComponents)\n",
    "valueChecker('corpusFolder', str, corpusFolder)\n",
    "valueChecker('removeItemsFile', str, removeItemsFile)\n",
    "\n",
    "# check tuples and internal values\n",
    "if type(labelTypes) != tuple:\n",
    "    print('labelTypes must be a tuple. Please fix to run script.')\n",
    "    sys.exit()\n",
    "else:\n",
    "    for lab in labelTypes:\n",
    "        valueChecker('labelType item', str, lab)\n",
    "\n",
    "if type(outputDimensions) != tuple:\n",
    "    print(f\"outputDimensions must be {tuple}. Please fix to run the script\")\n",
    "else:\n",
    "    for d in outputDimensions:\n",
    "        valueChecker(\"outerDimension value\", (float, int), d)\n",
    "\n",
    "# Load in external files\n",
    "try:\n",
    "    removeItems = []\n",
    "    with open(removeItemsFile, \"r\", encoding='utf8') as rf:\n",
    "        removeItems = [item.strip() for item in rf.read().split(\"\\n\") if item != \"\"]\n",
    "except FileNotFoundError:\n",
    "    print(f\"No file named {removeItemsFile} found. Please check filename or create the file.\")\n",
    "    sys.exit()\n",
    "\n",
    "if limitVocab == True:\n",
    "    valueChecker('limitVocabularyFile', str, limitVocabularyFile)\n",
    "    try:\n",
    "        limitVocabulary = [] \n",
    "        with open(limitVocabularyFile, \"r\", encoding='utf8') as rf:\n",
    "            limitVocabulary = [item.strip() for item in rf.read().split(\"\\n\") if item != \"\"]\n",
    "        if commonWords:\n",
    "            print(f\"You are limiting analysis to the {commonWords} most common words but also using a set vocabulary.\")\n",
    "            print(\"If you want to avoid unexpected behavior, set commonWords to None when limiting vocab.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"No file named {limitVocabularyFile} found. Please check filename or create the file\")\n",
    "        print(\"Defaulting to no limit on the vocabulary\")       \n",
    "        limitVocabulary = None\n",
    "else:\n",
    "    limitVocabulary = None\n",
    "\n",
    "# Ensure corpus folder exists\n",
    "if not os.path.isdir(corpusFolder):\n",
    "    print(f\"Could not find the corpus folder '{corpusFolder}'. Please double check.\")\n",
    "    sys.exit()\n",
    "\n",
    "######################\n",
    "# Load extra modules #\n",
    "######################\n",
    "\n",
    "# Extra modules will be loaded if you want to parse into words.\n",
    "if tokenizeMethod == \"word\":\n",
    "    try:\n",
    "        import jieba\n",
    "    except ImportError:\n",
    "        print(\"For word tokenizing, you will need to install the jieba library\")\n",
    "        print(\"You can do so by running the following command (you may need to run as admin):\")\n",
    "        print(\"pip install jieba\")\n",
    "        sys.exit()\n",
    "\n",
    "########################\n",
    "# Function definitions #\n",
    "########################\n",
    "\n",
    "# Function to clean the text. Remove desired characters and white space.\n",
    "def clean(text, removeitems):\n",
    "    for item in removeitems:\n",
    "        text = text.replace(item, \"\")\n",
    "    text = re.sub(\"\\s+\", \"\", text)\n",
    "    return text\n",
    "\n",
    "# Function to tokenize the text. This seperates the tokens with spaces\n",
    "def tokenize(text, tm = tokenizeMethod):\n",
    "    if tm == \"char\":\n",
    "        text = \" \".join(list(text))\n",
    "    elif tm == \"word\":\n",
    "        # this uses the jieba library, but there are other good options\n",
    "        # the best probably being stanford's parsers\n",
    "        # this will be VERY slow\n",
    "        text = \" \".join(jieba.cut(text))  \n",
    "    else:\n",
    "        print(\"Set tokenizationMethod to either char or word\")\n",
    "        sys.exit()\n",
    "    return text\n",
    "\n",
    "##############\n",
    "# Load Texts #\n",
    "##############\n",
    "\n",
    "print(\"Loading, cleaning, and tokenizing\")\n",
    "# Go through each document in the corpus folder and save info to lists\n",
    "texts = []\n",
    "labels = []\n",
    "\n",
    "for root, dirs, files in os.walk(corpusFolder):\n",
    "    for i, f in enumerate(files):\n",
    "        # add the labels to the label list\n",
    "        labels.append(f[:-4].split(\"_\"))\n",
    "\n",
    "        # Open the text, clean it, and tokenize it\n",
    "        with open(os.path.join(root,f),\"r\", encoding='utf8') as rf:\n",
    "            texts.append(tokenize(clean(rf.read(), removeItems)))\n",
    "        \n",
    "        if i == len(files) - 1:\n",
    "            print(f\"\\r{i+1} of {len(files)} processed\", end='\\n', flush=True)\n",
    "        else:\n",
    "            print(f\"\\r{i+1} of {len(files)} processed\", end='', flush=True)\n",
    "\n",
    "####################\n",
    "# Perform Analysis #\n",
    "####################\n",
    "\n",
    "print(\"Vectorizing\")\n",
    "countVectorizer = TfidfVectorizer(max_features=commonWords, use_idf=False, vocabulary=limitVocabulary,  analyzer='word', token_pattern='\\S+', ngram_range=(ngrams, ngrams))\n",
    "countMatrix = countVectorizer.fit_transform(texts)\n",
    "print(\"Normalizing values\")\n",
    "countMatrix = normalize(countMatrix)\n",
    "countMatrix = countMatrix.toarray()\n",
    "\n",
    "print(\"Performing PCA\")\n",
    "# Lets perform PCA on the countMatrix:\n",
    "pca = PCA(n_components=pcaComponents)\n",
    "myPCA = pca.fit_transform(countMatrix)\n",
    "\n",
    "\n",
    "##############\n",
    "# Plot Setup #\n",
    "##############\n",
    "\n",
    "# Set the font\n",
    "if platform.system() == \"Darwin\":\n",
    "    font = matplotlib.font_manager.FontProperties(fname=\"/System/Library/Fonts/STHeiti Medium.ttc\")\n",
    "    matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "elif platform.system() == \"Windows\":\n",
    "    font = matplotlib.font_manager.FontProperties(fname=\"C:\\\\Windows\\\\Fonts\\\\simsun.ttc\")\n",
    "elif platform.system() == \"Linux\":\n",
    "    # This assumes you have wqy zenhei installed\n",
    "    font = matplotlib.font_manager.FontProperties(fname=\"/usr/share/fonts/truetype/wqy/wqy-zenhei.ttc\")\n",
    "\n",
    "print(\"Setting plot info\")\n",
    "# set the plot size\n",
    "plt.figure(figsize=outputDimensions)\n",
    "\n",
    "# find all the unique values for each of the label types\n",
    "uniqueLabelValues = [set() for i in range(len(labelTypes))]\n",
    "for labelList in labels:\n",
    "    for i, label in enumerate(labelList):\n",
    "        uniqueLabelValues[i].add(label)\n",
    "\n",
    "# create color dictionaries for all labels\n",
    "colorDictionaries = []\n",
    "for uniqueLabels in uniqueLabelValues:\n",
    "    colorpalette = sns.color_palette(\"husl\",len(uniqueLabels)).as_hex()\n",
    "    colorDictionaries.append(dict(zip(uniqueLabels,colorpalette)))\n",
    "\n",
    "# Now we need the Unique Labels\n",
    "uniqueColorLabels = list(uniqueLabelValues[colorValue])\n",
    "# Let's get a number for each class\n",
    "numberForClass = [i for i in range(len(uniqueColorLabels))]\n",
    "\n",
    "# Make a dictionary! This is new sytax for us! It just makes a dictionary where\n",
    "# the keys are the unique years and the values are found in numberForClass\n",
    "labelForClassNumber = dict(zip(uniqueColorLabels,numberForClass))\n",
    "\n",
    "# Let's make a new representation for each document that is just these integers\n",
    "# and it needs to be a numpy array\n",
    "textClass = np.array([labelForClassNumber[lab[colorValue]] for lab in labels])\n",
    "\n",
    "\n",
    "# Make a list of the colors\n",
    "colors = [colorDictionaries[colorValue][lab] for lab in uniqueColorLabels]\n",
    "\n",
    "if hidePoints:\n",
    "    pointSize = 0\n",
    "\n",
    "###################\n",
    "# Create the plot #\n",
    "###################\n",
    "\n",
    "print(\"Plotting texts\")\n",
    "for col, classNumber, lab in zip(colors, numberForClass, uniqueColorLabels):\n",
    "    plt.scatter(myPCA[textClass==classNumber,0],myPCA[textClass==classNumber,1],label=lab,c=col, s=pointSize)\n",
    "\n",
    "# Let's label individual points so we know WHICH document they are\n",
    "if pointLabels:\n",
    "    print(\"Adding Labels\")\n",
    "    for lab, datapoint in zip(labels, myPCA):\n",
    "        plt.annotate(str(lab[labelValue]),xy=datapoint, fontproperties=font)\n",
    "\n",
    "# Let's graph component loadings\n",
    "vocabulary = countVectorizer.get_feature_names()\n",
    "loadings = pca.components_\n",
    "if plotLoadings:\n",
    "    print(\"Rendering Loadings\")    \n",
    "    for i, word in enumerate(vocabulary):\n",
    "        plt.annotate(word, xy=(loadings[0, i], loadings[1,i]), fontproperties=font)\n",
    "    \n",
    "\n",
    "# Let's add a legend! matplotlib will make this for us based on the data we \n",
    "# gave the scatter function.\n",
    "plt.legend(prop=font)\n",
    "plt.savefig(outputFile)\n",
    "\n",
    "\n",
    "############################################\n",
    "# Output data for JavaScript Visualization #\n",
    "############################################\n",
    "\n",
    "data = []\n",
    "for datapoint in myPCA:\n",
    "    pcDict = {}\n",
    "    for i, dp in enumerate(datapoint):\n",
    "        pcDict[f\"PC{str(i + 1)}\"] = dp\n",
    "    data.append(pcDict)\n",
    "\n",
    "jsLoadings = []\n",
    "for i, word in enumerate(vocabulary):\n",
    "    temploading = {}\n",
    "    for j,dp in enumerate(loadings):\n",
    "        temploading[f\"PC{str(j+1)}\"] = dp[i]\n",
    "    jsLoadings.append([word, temploading])\n",
    "\n",
    "colorDictionaryList = []\n",
    "for cd in colorDictionaries:\n",
    "    cdlist = [v for v in cd.values()]\n",
    "    colorDictionaryList.append(cdlist)\n",
    "print(colorDictionaryList[0])\n",
    "\n",
    "colorstrings = json.dumps(colorDictionaryList)\n",
    "labelstrings = json.dumps(labels)\n",
    "valuetypes = json.dumps([k for k in data[0].keys()])\n",
    "datastrings = json.dumps(data)\n",
    "\n",
    "limitedlabeltypes = []\n",
    "for i, t in enumerate(labelTypes):\n",
    "    if len(uniqueLabelValues[i]) <= 20:\n",
    "        limitedlabeltypes.append(t)\n",
    "\n",
    "cattypestrings = json.dumps(limitedlabeltypes)\n",
    "loadingstrings = json.dumps(jsLoadings)\n",
    "stringlist = [f\"var colorDictionaries = {colorstrings};\", f\"var labels = {labelstrings};\",\n",
    "            f\"var data = {datastrings};\", f\"var categoryTypes = {list(labelTypes)};\", \n",
    "            f\"var loadings = {jsLoadings};\", f\"var valueTypes = {valuetypes};\",\n",
    "            f\"var limitedCategories = {limitedlabeltypes};\"]\n",
    "\n",
    "\n",
    "with open(\"data.js\", \"w\", encoding=\"utf8\") as wf:\n",
    "    wf.write(\"\\n\".join(stringlist))\n",
    "\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
